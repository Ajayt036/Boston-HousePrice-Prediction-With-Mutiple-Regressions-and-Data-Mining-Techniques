{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd72a29f",
   "metadata": {},
   "source": [
    "## Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb462918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from mord import LogisticIT\n",
    "\n",
    "from dmba import classificationSummary, regressionSummary\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1d4ee",
   "metadata": {},
   "source": [
    "### 1A. Create a boston_df data frame by uploading the original data set into Python. Determine and present in this report the data frame dimensions, i.e., number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490c2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows  506\n",
      "Number of Columns:  14\n"
     ]
    }
   ],
   "source": [
    "boston_df = pd.read_csv(\"BostonHousing.csv\")\n",
    "print(\"Number of Rows \",boston_df.shape[0])\n",
    "print(\"Number of Columns: \",boston_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5357383",
   "metadata": {},
   "source": [
    "### 1B. Display in Python the column titles. If some of them contain two (or more) words, convert them into one-word titles, and present the modified titles in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b9af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified column titles with no space and one word for titles: \n",
      "\n",
      "Index(['CRIME', 'ZONE', 'INDUST', 'CHAR_RIV', 'NIT_OXIDE', 'ROOMS', 'AGE',\n",
      "       'DISTANCE', 'RADIAL', 'TAX', 'ST_RATIO', 'LOW_STAT', 'MVALUE',\n",
      "       'C_MVALUE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# We will strip trailing spaces and replace the remaining spaces with an underscore _. Instead of using \n",
    "# the `rename` method, we  create a modified copy of `columns` and assign to the `columns` field of the dataframe.\n",
    "\n",
    "print('Modified column titles with no space and one word for titles:',\"\\n\")\n",
    "boston_df.columns = [s.strip().replace(' ', '_') for s in boston_df.columns]\n",
    "print(boston_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183da44",
   "metadata": {},
   "source": [
    "### 1C. Display in Python column data types. If some of them are listed as “object’, convert them into dummy variables, and provide in your report the modified list of column titles with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbb1857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIME        float64\n",
      "ZONE         float64\n",
      "INDUST       float64\n",
      "CHAR_RIV      object\n",
      "NIT_OXIDE    float64\n",
      "ROOMS        float64\n",
      "AGE          float64\n",
      "DISTANCE     float64\n",
      "RADIAL         int64\n",
      "TAX            int64\n",
      "ST_RATIO     float64\n",
      "LOW_STAT     float64\n",
      "MVALUE       float64\n",
      "C_MVALUE      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(boston_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbbc77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column with Object data type in the dataset are: \n",
      "CHAR_RIV\n",
      "C_MVALUE\n",
      "\n",
      "Category levels and changed variable type of CHAR_RIV:\n",
      "Index(['N', 'Y'], dtype='object')\n",
      "category\n",
      "\n",
      "Category levels and changed variable type of C_MVALUE:\n",
      "Index(['No', 'Yes'], dtype='object')\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "# WE Can see only two columns have object data type:\n",
    "print(\"Column with Object data type in the dataset are: \")\n",
    "for i in boston_df.columns:\n",
    "    if boston_df[i].dtype == \"O\":\n",
    "        print(i)\n",
    "            \n",
    "\n",
    "# Lets change these variable type to 'category'.\n",
    "boston_df.CHAR_RIV = boston_df.CHAR_RIV.astype('category')\n",
    "boston_df.C_MVALUE = boston_df.C_MVALUE.astype('category')\n",
    "\n",
    "\n",
    "# Display category levels (attributes) and category type FOR CHAR_RIV\n",
    "print(\"\")\n",
    "print('Category levels and changed variable type of CHAR_RIV:')\n",
    "print(boston_df.CHAR_RIV.cat.categories) \n",
    "print(boston_df.CHAR_RIV.dtype)\n",
    "\n",
    "\n",
    "# Display category levels (attributes) and category type FOR C_MVALUE\n",
    "print(\"\")\n",
    "print('Category levels and changed variable type of C_MVALUE:')\n",
    "print(boston_df.C_MVALUE.cat.categories) \n",
    "print(boston_df.C_MVALUE.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a8b297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIME', 'ZONE', 'INDUST', 'NIT_OXIDE', 'ROOMS', 'AGE', 'DISTANCE',\n",
       "       'RADIAL', 'TAX', 'ST_RATIO', 'LOW_STAT', 'MVALUE', 'CHAR_RIV_Y',\n",
       "       'C_MVALUE_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets convert these categorical column to Dummy vraibales\n",
    "boston_df = pd.get_dummies(boston_df, prefix_sep='_', drop_first=True)\n",
    "boston_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d21e06",
   "metadata": {},
   "source": [
    "### 2A. Develop in Python the outcome and predictor variables, partition the data set (60% for training and 40% for validation partitions), display in Python and present in your report the first five records of the training partition. Then, using the StandardScaler() function, develop the scaled predictors for training and validation partitions. Display in Python and provide in your report the first five records of the scaled training partition. Present a brief explanation of what the scaled values mean and how they are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d174dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records of Training Dataset are: \n",
      "       CRIME  ZONE  INDUST  NIT_OXIDE  ROOMS   AGE  DISTANCE  RADIAL  TAX  \\\n",
      "452  5.09017   0.0   18.10      0.713  6.297  91.8    2.3682      24  666   \n",
      "346  0.06162   0.0    4.39      0.442  5.898  52.3    8.0136       3  352   \n",
      "295  0.12932   0.0   13.92      0.437  6.678  31.1    5.9604       4  289   \n",
      "88   0.05660   0.0    3.41      0.489  7.007  86.3    3.4217       2  270   \n",
      "322  0.35114   0.0    7.38      0.493  6.041  49.9    4.7211       5  287   \n",
      "\n",
      "     ST_RATIO  LOW_STAT  CHAR_RIV_Y  C_MVALUE_Yes  \n",
      "452      20.2     17.27           0             0  \n",
      "346      18.8     12.67           0             0  \n",
      "295      16.0      6.27           0             0  \n",
      "88       17.8      5.50           0             0  \n",
      "322      19.6      7.70           0             0   \n",
      "\n",
      "\n",
      "\n",
      "First 5 records of Standardize Training Dataset are: \n",
      "      CRIME      ZONE    INDUST  NIT_OXIDE     ROOMS       AGE  DISTANCE  \\\n",
      "0  0.145983 -0.481603  1.005718   1.306155  0.083382  0.802970 -0.688439   \n",
      "1 -0.419174 -0.481603 -1.003112  -0.954625 -0.510679 -0.583845  1.948197   \n",
      "2 -0.411566 -0.481603  0.393252  -0.996337  0.650643 -1.328160  0.989267   \n",
      "3 -0.419739 -0.481603 -1.146704  -0.562534  1.140482  0.609869 -0.196411   \n",
      "4 -0.386635 -0.481603 -0.565008  -0.529165 -0.297770 -0.668107  0.410463   \n",
      "\n",
      "     RADIAL       TAX  ST_RATIO  LOW_STAT  CHAR_RIV_Y  C_MVALUE_Yes  \n",
      "0  1.662458  1.529763  0.830829  0.559947   -0.293294     -0.449868  \n",
      "1 -0.755421 -0.337176  0.188515 -0.061340   -0.293294     -0.449868  \n",
      "2 -0.640284 -0.711753 -1.096113 -0.925739   -0.293294     -0.449868  \n",
      "3 -0.870558 -0.824721 -0.270281 -1.029737   -0.293294     -0.449868  \n",
      "4 -0.525147 -0.723644  0.555552 -0.732600   -0.293294     -0.449868  \n"
     ]
    }
   ],
   "source": [
    "#Partitioning Data\n",
    "\n",
    "# Identify predictors(col names) and outcome(target Name) of the regression model]\\\n",
    "# These are just names ( not data)\n",
    "predictors = ['CRIME', 'ZONE', 'INDUST', 'NIT_OXIDE', 'ROOMS', 'AGE', 'DISTANCE',\n",
    "       'RADIAL', 'TAX', 'ST_RATIO', 'LOW_STAT', 'CHAR_RIV_Y',\n",
    "       'C_MVALUE_Yes']\n",
    "\n",
    "outcome = 'MVALUE'\n",
    "\n",
    "\n",
    "# Identify X and y variables for regression and partition data\n",
    "# using 60% of records for training and 40% for validation (test_size=0.4). \n",
    "X = boston_df[predictors]\n",
    "Y = boston_df[outcome]\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, Y, test_size=0.4, random_state=1)\n",
    "\n",
    "print(\"First 5 records of Training Dataset are: \")\n",
    "print(train_X.head(5),\"\\n\"*3)\n",
    "\n",
    "#Standardize All predictor value in train_x and valid_x with standard scaler\n",
    "sc_X = StandardScaler()\n",
    "sc_X.fit(train_X)\n",
    "train_X_sc = pd.DataFrame(sc_X.transform(train_X), columns= train_X.columns)\n",
    "valid_X_sc = pd.DataFrame(sc_X.transform(valid_X), columns= valid_X.columns)\n",
    "\n",
    "print(\"First 5 records of Standardize Training Dataset are: \")\n",
    "print(train_X_sc.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08c281",
   "metadata": {},
   "source": [
    "### 2B. Train a neural network model using MLPRegressor() with the scaled training data set and the following parameters: hidden_layer_sizes=9, solver=’lbfgs’, max_iter=10000, and random_state=1. Identify and display in Python the final intercepts and network weights of this model. Provide these intercepts and weights in your report and briefly explain what the values of intercepts in the first and second arrays mean. Also, briefly explain what the values of weights in the first and second arrays mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f76de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Intercepts for Boston Housing Neural Network Model\n",
      "[array([ 2.26914474,  4.18675342, -1.50466092, -0.95080817,  0.32205673,\n",
      "        0.49141424, -0.81237211,  0.89649621,  2.52709405]), array([-11.97658924])]\n",
      "\n",
      "Network Weights for Boston Housing Neural Network Model\n",
      "[array([[-0.36367279,  0.52687745, -0.00964011, -1.5129948 ,  1.31769872,\n",
      "        -0.41850542, -1.53776369, -0.62480937, -1.90841567],\n",
      "       [ 0.66134206, -1.23924591,  0.28794966,  2.11953871,  1.07434276,\n",
      "         0.43220725, -4.11086338,  0.40334917, -0.08228966],\n",
      "       [ 0.59676784,  0.58331396,  2.48943055,  0.90207757, -1.28237762,\n",
      "         0.42433014, -1.95774694,  0.95234875,  1.23319412],\n",
      "       [-2.20869382, -1.37160062, -0.98568213,  0.36329227, -0.6196563 ,\n",
      "        -0.30782417,  1.05946611,  0.83457418,  2.71468783],\n",
      "       [ 0.16998566, -0.09067205, -1.7444194 ,  0.11677319,  1.80120993,\n",
      "         0.81352311, -0.01783085, -2.03768459,  0.23723986],\n",
      "       [-0.27165058, -2.52251129, -1.29506367,  0.3880619 ,  0.44373225,\n",
      "         0.51115637,  0.8306126 ,  2.20731332,  0.09551513],\n",
      "       [-1.74438324, -1.35593869,  0.74218103, -1.21093971, -0.69583533,\n",
      "        -1.5105213 , -2.02212252,  3.26384001,  2.69895918],\n",
      "       [ 1.78857118,  0.22470477,  0.04506329,  2.44667139, -2.59695682,\n",
      "         3.58357297,  2.76451342,  2.06536825, -2.18559373],\n",
      "       [-2.74560654, -0.75960224, -1.80703499, -0.49290849,  2.18655319,\n",
      "        -1.41363973,  0.08668975, -0.95125975,  1.66200652],\n",
      "       [-0.51989512, -1.10225497, -1.92888649, -0.44066122,  2.16479034,\n",
      "        -1.2403606 ,  1.59544772,  0.83730668, -1.14664323],\n",
      "       [ 0.18569259, -1.05978838, -1.49423995,  0.57938554, -0.87388064,\n",
      "        -0.65901279,  1.50251509, -1.80353844, -0.48843375],\n",
      "       [-0.05494451, -0.8172153 , -0.42448891,  0.43840623,  0.79331301,\n",
      "        -1.1406599 ,  0.14764358,  0.05219393,  0.55909692],\n",
      "       [-1.91183736,  1.16460099, -0.03811763, -2.00838446,  1.32188124,\n",
      "         2.55833196, -0.02954175,  0.43669783, -0.57348116]]), array([[ 3.39687967],\n",
      "       [ 2.13203837],\n",
      "       [-2.33602904],\n",
      "       [-2.86800963],\n",
      "       [ 1.64344769],\n",
      "       [ 1.53156148],\n",
      "       [ 2.48111618],\n",
      "       [ 1.27104678],\n",
      "       [ 2.56836215]])]\n"
     ]
    }
   ],
   "source": [
    "boston_reg = MLPRegressor(hidden_layer_sizes=(9), \n",
    "                solver='lbfgs', max_iter=10000, random_state=1)\n",
    "\n",
    "boston_reg.fit(train_X_sc, train_y)\n",
    "\n",
    "# Display network structure with the final values of intercepts (Theta) and weights (W).\n",
    "print('Final Intercepts for Boston Housing Neural Network Model')\n",
    "print(boston_reg.intercepts_)\n",
    "\n",
    "print()\n",
    "print('Network Weights for Boston Housing Neural Network Model')\n",
    "print(boston_reg.coefs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ecba4",
   "metadata": {},
   "source": [
    "### 2C.  Using the developed neural network model, make in Python predictions for the outcome variable (MVALUE) using the scaled validation predictors. Based on these predictions, develop and display in Python a table for the first five validation records that contain actual and predicted median prices (MVALUE), and their residuals. Present this table in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1e5bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for MVALUE for Validation Partition\n",
      "     Actual  Prediction  Residual\n",
      "307    28.2       29.63     -1.43\n",
      "343    23.9       23.49      0.41\n",
      "47     16.6       17.80     -1.20\n",
      "67     22.0       18.73      3.27\n",
      "362    20.8       25.30     -4.50\n"
     ]
    }
   ],
   "source": [
    "# Make 'MVALUE' predictions for validation set using our Model\n",
    "price_pred = np.round(boston_reg.predict(valid_X_sc), decimals=2)\n",
    "\n",
    "price_pred_result = pd.DataFrame({'Actual': valid_y, \n",
    "                'Prediction': price_pred, 'Residual': valid_y-price_pred})\n",
    "\n",
    "print('Predictions for MVALUE for Validation Partition')\n",
    "print(price_pred_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c3009",
   "metadata": {},
   "source": [
    "### 2D. Identify and display in Python the common accuracy measures for training and validation partitions. Provide and compare these accuracy measures in your report and assess a possibility of overfitting. Would you recommend applying this neural network model for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da0bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measures for Training Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : -0.0034\n",
      "       Root Mean Squared Error (RMSE) : 1.5617\n",
      "            Mean Absolute Error (MAE) : 1.1368\n",
      "          Mean Percentage Error (MPE) : -0.8274\n",
      "Mean Absolute Percentage Error (MAPE) : 6.0681\n",
      "\n",
      "Accuracy Measures for Validation Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : -0.0912\n",
      "       Root Mean Squared Error (RMSE) : 3.1675\n",
      "            Mean Absolute Error (MAE) : 2.2668\n",
      "          Mean Percentage Error (MPE) : -3.0502\n",
      "Mean Absolute Percentage Error (MAPE) : 11.6748\n"
     ]
    }
   ],
   "source": [
    "# Neural network model accuracy measures for training and validation partitions. \n",
    "print('Accuracy Measures for Training Partition for Neural Network')\n",
    "regressionSummary(train_y, boston_reg.predict(train_X_sc))\n",
    "\n",
    "# Identify and display neural network accuracy measures for validation partition.\n",
    "print()\n",
    "print('Accuracy Measures for Validation Partition for Neural Network')\n",
    "regressionSummary(valid_y, boston_reg.predict(valid_X_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62846bbe",
   "metadata": {},
   "source": [
    "### 3A. Use in Python GridSearchCV() function to identify the best number of nodes for the hidden layer in the Boston Housing neural network model. For that, consider the hidden_layer_sizes parameter in a range from 2 to 20. Provide in your report the best score and best parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b6ed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:0.8759\n",
      "Best parameter:  {'hidden_layer_sizes': 2}\n"
     ]
    }
   ],
   "source": [
    "# Identify grid search parameters. \n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': list(range(2, 21)), \n",
    "}\n",
    "\n",
    "# Utilize GridSearchCV() to identify the best number of nodes in the hidden layer. \n",
    "gridSearch = GridSearchCV(MLPRegressor(solver='lbfgs', max_iter=10000, random_state=1), \n",
    "                          param_grid, cv=5, n_jobs=-1, return_train_score=True)\n",
    "gridSearch.fit(train_X_sc, train_y)\n",
    "\n",
    "# Display the best score and best parament value.\n",
    "print(f'Best score:{gridSearch.best_score_:.4f}')\n",
    "print('Best parameter: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00970ee1",
   "metadata": {},
   "source": [
    "### 3B. Train an improved neural network model using MLPRegressor() with the scaled training data set and the best identified value of the parameter from the previous question. The rest of the parameters remain the same as in model developed in 2b. Present in your report the final intercepts and network weights of the improved neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "328b8eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Intercepts for improved Neural Network Model\n",
      "[array([-5.59839276,  8.69124415]), array([6.60346469])]\n",
      "\n",
      "Network Weights for improved Neural Network Model\n",
      "[array([[-0.20769012, -1.78999663],\n",
      "       [-0.31789029,  0.2530354 ],\n",
      "       [ 3.68804841, -0.23745973],\n",
      "       [-0.39979107, -0.36498108],\n",
      "       [-1.54682674,  2.3322485 ],\n",
      "       [ 0.12819544, -0.90563125],\n",
      "       [ 0.31696069, -1.11355164],\n",
      "       [ 3.16426975,  0.13759987],\n",
      "       [ 1.58396291, -1.3793737 ],\n",
      "       [-2.27344137, -0.55228335],\n",
      "       [-1.32734466, -0.43485492],\n",
      "       [-0.01351816,  0.17987294],\n",
      "       [ 3.13443234,  1.31033442]]), array([[2.40608879],\n",
      "       [1.58695947]])]\n"
     ]
    }
   ],
   "source": [
    "boston_reg_imp = MLPRegressor(hidden_layer_sizes=2, \n",
    "                solver='lbfgs', max_iter=10000, random_state=1)\n",
    "boston_reg_imp.fit(train_X_sc, train_y)\n",
    "\n",
    "\n",
    "print('Final Intercepts for improved Neural Network Model')\n",
    "print(boston_reg_imp.intercepts_)\n",
    "\n",
    "print()\n",
    "print('Network Weights for improved Neural Network Model')\n",
    "print(boston_reg_imp.coefs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca7c60",
   "metadata": {},
   "source": [
    "### 3C. Identify and display in Python the common accuracy measures for the training and validation partitions with the improved neural network model. Provide and compare these accuracy measures in your report and assess a possibility of overfitting. Would you recommend applying this neural network model for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88902a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measures for Training Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 0.0013\n",
      "       Root Mean Squared Error (RMSE) : 2.6108\n",
      "            Mean Absolute Error (MAE) : 2.0053\n",
      "          Mean Percentage Error (MPE) : -1.8353\n",
      "Mean Absolute Percentage Error (MAPE) : 10.4838\n",
      "\n",
      "Accuracy Measures for Validation Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 0.0295\n",
      "       Root Mean Squared Error (RMSE) : 3.0570\n",
      "            Mean Absolute Error (MAE) : 2.2651\n",
      "          Mean Percentage Error (MPE) : -2.3393\n",
      "Mean Absolute Percentage Error (MAPE) : 11.4870\n"
     ]
    }
   ],
   "source": [
    "# Neural network model accuracy measures for training and validation partitions with improved Model \n",
    "print('Accuracy Measures for Training Partition for Neural Network')\n",
    "regressionSummary(train_y, boston_reg_imp.predict(train_X_sc))\n",
    "\n",
    "# Identify and display neural network accuracy measures for validation partition.\n",
    "print()\n",
    "print('Accuracy Measures for Validation Partition for Neural Network')\n",
    "regressionSummary(valid_y, boston_reg_imp.predict(valid_X_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b9faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
